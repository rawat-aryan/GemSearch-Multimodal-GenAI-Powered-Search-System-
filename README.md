# ğŸ” GemSearch: GenAI-Powered Multimodal Search System

GemSearch is a **multimodal semantic search engine** that allows you to search using **text**, **image**, or **audio** inputs. It uses state-of-the-art open-source models for embedding and retrieval, and integrates them into a unified FastAPI backend with a Streamlit frontend.

---

## ğŸš€ Features

- âœ… **Text Search** via Sentence Transformers
- ğŸ–¼ï¸ **Image Search** using OpenAI CLIP embeddings
- ğŸ”Š **Audio Search** using Whisper transcription + text embedding
- ğŸ§  Ready to integrate **Gemini Pro/Vision** APIs for GenAI workflows
- ğŸ“¦ Lightweight, modular structure with FastAPI & Streamlit
- ğŸŒ Run locally or deploy to Codespaces, Docker, or Cloud

---

## ğŸ—‚ï¸ Project Structure

GemSearch/
â”‚
â”œâ”€â”€ app/ # Core backend logic

â”‚ â”œâ”€â”€ main.py # FastAPI entrypoint

â”‚ â”œâ”€â”€ search_text.py # SentenceTransformer-based text search

â”‚ â”œâ”€â”€ search_image.py # CLIP-based image search

â”‚ â”œâ”€â”€ search_audio.py # Whisper-based audio transcription + search

â”‚
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ streamlit_app.py # Multimodal search interface
â”‚
â”œâ”€â”€ models/ # (Optional) Store downloaded model weights
â”œâ”€â”€ data/ # (Optional) Sample input files
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project overview and guide


---

## ğŸ§  Models Used

| Modality | Task                     | Model                                  |
|----------|--------------------------|----------------------------------------|
| Text     | Semantic Search          | `sentence-transformers/all-MiniLM-L6-v2` |
| Image    | Embedding + Caption Match| `OpenAI CLIP (ViT-B/32)`               |
| Audio    | Speech to Text           | `openai/whisper (base)`                |
| Optional | GenAI Answering          | `Gemini Pro / Gemini Vision` API       |

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/GemSearch.git
cd GemSearch


2. Create Virtual Environment

python -m venv venv
source venv/bin/activate         # On Windows: venv\Scripts\activate
3. Install Dependencies
pip install --upgrade pip
pip install -r requirements.txt


4. Fix CLIP Module (If needed)
If clip.load(...) throws an error:

pip uninstall clip -y
pip install git+https://github.com/openai/CLIP.git


â–¶ï¸ Running the Project
Start the FastAPI Backend

uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

By default, the API runs at:
http://localhost:8000/docs â†’ FastAPI Swagger UI
http://localhost:8000/search_text â†’ POST endpoint for queries

Start the Streamlit Frontend
Open a new terminal and run:

streamlit run ui/streamlit_app.py
This will open the web app at:
http://localhost:8501/



ğŸ” How It Works
Text Search
Takes user query â†’ embeds with SentenceTransformer

Computes cosine similarity with sample corpus embeddings

Returns top matching entries

Image Search
Upload image â†’ CLIP extracts visual embedding

Compared with a set of caption embeddings

Returns top-matching captions

Audio Search
Upload audio â†’ Whisper transcribes to text

The transcript is processed by text search logic

ğŸ§ª Demo Inputs
You can test with:

dog (text)

Any photo (e.g., landscape, dog, computer)

Voice audio saying "A dog playing in the park"

ğŸ”§ Optional: Gemini API Integration
To use Gemini Pro or Vision:

Install google-generativeai

Configure with your API Key

Wrap text/image/audio content via Gemini model prompt
(Ask me for code snippets if you'd like this added.)

ğŸ“Œ Future Improvements
Add Gemini Pro response fallback

Store indexed corpora in vector DB (e.g., FAISS, ChromaDB)

Streamlined Docker deployment

Upload video and extract multimodal content

ğŸ§‘â€ğŸ’» Author
Aryan Rawat
ML Engineer | CV & MLOps | FastAPI | GCP
